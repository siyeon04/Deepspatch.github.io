from fastapi import FastAPI, File, UploadFile, HTTPException, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn
import cv2
import numpy as np
import tempfile
import os
from pathlib import Path
import torch
import torch.nn as nn
from torchvision import transforms
import subprocess
import re
import librosa
from PIL import Image

# TensorFlow/Keras import
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers, models
    TF_AVAILABLE = True
    print("âœ… TensorFlow ì‚¬ìš© ê°€ëŠ¥")
    
    # GANomaly ì»¤ìŠ¤í…€ í´ë˜ìŠ¤ ì •ì˜
    class GANomaly(keras.Model):
        def __init__(self, generator=None, discriminator=None, feature_extractor=None, g_encoder=None, **kwargs):
            super(GANomaly, self).__init__(**kwargs)
            self.generator = generator
            self.discriminator = discriminator
            self.feature_extractor = feature_extractor
            self.g_encoder = g_encoder
        
        def call(self, inputs, training=None):
            if self.generator is not None:
                return self.generator(inputs, training=training)
            return inputs
    
    print("âœ… GANomaly í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ")
    
except ImportError:
    TF_AVAILABLE = False
    print("âš ï¸  TensorFlow ì—†ìŒ - ì˜¤ë””ì˜¤ ëª¨ë¸ ë¡œë“œ ë¶ˆê°€")

app = FastAPI(title="ë”¥í˜ì´í¬ íƒì§€ API")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== Config í´ë˜ìŠ¤ ì •ì˜ ====================

class Config:
    """GANomaly ì„¤ì • í´ë˜ìŠ¤"""
    def __init__(self):
        self.isize = 256
        self.nc = 3
        self.nz = 100

# ==================== ì„¤ì • ====================

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸  PyTorch device: {device}")

# ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
VIDEO_MODEL_PATH = "models/ganomaly_deepfake_model_2.pth"  # PyTorch
AUDIO_MODEL_PATH = "models/ganomaly_model_full_dataset.h5"  # TensorFlow/Keras

IMAGE_SIZE = 256  # ë¹„ë””ì˜¤ ì…ë ¥ í¬ê¸°
AUDIO_IMAGE_SIZE = 256  # ì˜¤ë””ì˜¤ spectrogram ì´ë¯¸ì§€ í¬ê¸°

# ==================== ë¹„ë””ì˜¤ ëª¨ë¸ ë¡œë“œ (PyTorch) ====================

video_model = None

try:
    print(f"\n{'='*60}")
    print("ğŸ“¦ ë¹„ë””ì˜¤ GANomaly ëª¨ë¸ ë¡œë“œ ì¤‘ (PyTorch)...")
    print(f"{'='*60}")
    
    checkpoint = torch.load(VIDEO_MODEL_PATH, map_location=device, weights_only=False)
    
    if isinstance(checkpoint, dict):
        print("âœ… ì²´í¬í¬ì¸íŠ¸ ë”•ì…”ë„ˆë¦¬ ë°œê²¬")
        
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        elif 'netg' in checkpoint:
            state_dict = checkpoint['netg']
        else:
            state_dict = checkpoint
        
        if 'opt' in checkpoint:
            opt = checkpoint['opt']
            if hasattr(opt, 'isize'):
                IMAGE_SIZE = opt.isize
                print(f"âœ… ì…ë ¥ í¬ê¸°: {IMAGE_SIZE}x{IMAGE_SIZE}")
    else:
        video_model = checkpoint
    
    if video_model is None:
        class DeepfakeDetectorWrapper(nn.Module):
            def __init__(self, state_dict):
                super().__init__()
                self.load_state_dict(state_dict, strict=False)
            
            def forward(self, x):
                return x
        
        video_model = DeepfakeDetectorWrapper(state_dict)
    
    video_model.to(device)
    video_model.eval()
    
    print("âœ… ë¹„ë””ì˜¤ ëª¨ë¸ ë¡œë“œ ì„±ê³µ! (PyTorch)")
    print(f"{'='*60}\n")
    
except Exception as e:
    print(f"\nâŒ ë¹„ë””ì˜¤ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("âš ï¸  ë¹„ë””ì˜¤ ë”ë¯¸ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.\n")

# ==================== ì˜¤ë””ì˜¤ ëª¨ë¸ ë¡œë“œ (TensorFlow/Keras) ====================

audio_model = None

if TF_AVAILABLE:
    try:
        print(f"\n{'='*60}")
        print("ğŸ“¦ ì˜¤ë””ì˜¤ GANomaly ëª¨ë¸ ë¡œë“œ ì¤‘ (TensorFlow/Keras)...")
        print(f"{'='*60}")
        
        # ë°©ë²• 1: Generator êµ¬ì¡° ì¬êµ¬ì„±
        try:
            print("  ğŸ”¨ ì „ì²´ GANomaly êµ¬ì¡° ì¬êµ¬ì„± ì¤‘...")
            
            # Generator Encoder êµ¬ì¡° (í•™ìŠµ ì½”ë“œì™€ ë™ì¼)
            from tensorflow.keras import regularizers
            
            height, width, channels = 128, 128, 1
            
            # ========== Encoder ==========
            input_layer_g_e = layers.Input(name='input_g_e', shape=(height, width, channels))
            x = layers.Conv2D(32, (5, 5), strides=(1, 1), padding='same', name='conv_1', kernel_regularizer=regularizers.l2(0.0001))(input_layer_g_e)
            x = layers.LeakyReLU(name='leaky_1')(x)
            x = layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', name='conv_2', kernel_regularizer=regularizers.l2(0.0001))(x)
            x = layers.BatchNormalization(name='norm_1')(x)
            x = layers.LeakyReLU(name='leaky_2')(x)
            x = layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same', name='conv_3', kernel_regularizer=regularizers.l2(0.0001))(x)
            x = layers.BatchNormalization(name='norm_2')(x)
            x = layers.LeakyReLU(name='leaky_3')(x)
            x = layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same', name='conv_4', kernel_regularizer=regularizers.l2(0.0001))(x)
            x = layers.BatchNormalization(name='norm_3')(x)
            x = layers.LeakyReLU(name='leaky_4')(x)
            x = layers.GlobalAveragePooling2D(name='g_encoder_output')(x)
            g_e = models.Model(inputs=input_layer_g_e, outputs=x, name='g_encoder')
            
            # ========== Decoder ==========
            input_layer_g_d = layers.Input(name='input_g_d', shape=g_e.output_shape[1:])
            y = layers.Dense(width // 8 * width // 8 * 128, name='dense')(input_layer_g_d)
            y = layers.Reshape((width // 8, width // 8, 128), name='de_reshape')(y)
            y = layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same', name='deconv_1', kernel_regularizer=regularizers.l2(0.0001))(y)
            y = layers.LeakyReLU(name='de_leaky_1')(y)
            y = layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', name='deconv_2', kernel_regularizer=regularizers.l2(0.0001))(y)
            y = layers.LeakyReLU(name='de_leaky_2')(y)
            y = layers.Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same', name='deconv_3', kernel_regularizer=regularizers.l2(0.0001))(y)
            y = layers.LeakyReLU(name='de_leaky_3')(y)
            y = layers.Conv2DTranspose(channels, (1, 1), strides=(1, 1), padding='same', name='decoder_deconv_output', kernel_regularizer=regularizers.l2(0.0001), activation='tanh')(y)
            g_d = models.Model(inputs=input_layer_g_d, outputs=y, name='g_decoder')
            
            # ========== Generator (Full) ==========
            input_layer_g = layers.Input(name='input_g', shape=(height, width, channels))
            latent_vector = g_e(input_layer_g)
            generated_image = g_d(latent_vector)
            generator = models.Model(inputs=input_layer_g, outputs=generated_image, name='generator')
            
            # ========== Discriminator (Feature Extractor í¬í•¨) ==========
            input_layer_d = layers.Input(name='input_d', shape=(height, width, channels))
            f = layers.Conv2D(32, (5, 5), strides=(1, 1), padding='same', name='f_conv_1', kernel_regularizer=regularizers.l2(0.0001))(input_layer_d)
            f = layers.LeakyReLU(name='f_leaky_1')(f)
            f = layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', name='f_conv_2', kernel_regularizer=regularizers.l2(0.0001))(f)
            f = layers.BatchNormalization(name='f_norm_1')(f)
            f = layers.LeakyReLU(name='f_leaky_2')(f)
            f = layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same', name='f_conv_3', kernel_regularizer=regularizers.l2(0.0001))(f)
            f = layers.BatchNormalization(name='f_norm_2')(f)
            f = layers.LeakyReLU(name='f_leaky_3')(f)
            f = layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same', name='f_conv_4', kernel_regularizer=regularizers.l2(0.0001))(f)
            f = layers.BatchNormalization(name='f_norm_3')(f)
            f = layers.LeakyReLU(name='feature_output')(f)
            feature_extractor = models.Model(inputs=input_layer_d, outputs=f, name='feature_extractor')
            
            d_output = layers.GlobalAveragePooling2D(name='glb_avg')(f)
            d_output = layers.Dense(1, activation='sigmoid', name='d_out')(d_output)
            discriminator = models.Model(inputs=input_layer_d, outputs=d_output, name='discriminator')
            
            print("  âœ… ì „ì²´ êµ¬ì¡° ì¬êµ¬ì„± ì™„ë£Œ (generator, discriminator, feature_extractor, g_encoder)")
            
            # GANomaly ëª¨ë¸ ìƒì„± (4ê°œ ì„œë¸Œëª¨ë¸ ëª¨ë‘ í¬í•¨)
            audio_model_full = GANomaly(
                generator=generator,
                discriminator=discriminator,
                feature_extractor=feature_extractor,
                g_encoder=g_e
            )
            
            # H5 íŒŒì¼ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ
            print("  ğŸ“¥ ê°€ì¤‘ì¹˜ ë¡œë“œ ì¤‘...")
            audio_model_full.load_weights(AUDIO_MODEL_PATH)
            print("  âœ… ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ")
            
            # Generatorë§Œ ì¶”ì¶œí•˜ì—¬ ì‚¬ìš©
            audio_model = audio_model_full.generator
            
            print("âœ… ì˜¤ë””ì˜¤ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            print(f"  ì…ë ¥ shape: {audio_model.input_shape}")
            print(f"  ì¶œë ¥ shape: {audio_model.output_shape}")
            
            AUDIO_IMAGE_SIZE = 128
            print(f"âœ… ì˜¤ë””ì˜¤ ì…ë ¥ í¬ê¸°: {AUDIO_IMAGE_SIZE}x{AUDIO_IMAGE_SIZE}")
            
        except Exception as e1:
            print(f"  âŒ êµ¬ì¡° ì¬êµ¬ì„± ì‹¤íŒ¨: {e1}")
            import traceback
            traceback.print_exc()
            raise Exception(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
        
        print("âœ… ì˜¤ë””ì˜¤ ëª¨ë¸ ë¡œë“œ ì„±ê³µ! (TensorFlow/Keras)")
        print(f"{'='*60}\n")
        
    except FileNotFoundError:
        print(f"\nâš ï¸  ì˜¤ë””ì˜¤ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {AUDIO_MODEL_PATH}")
        print("âš ï¸  ì˜¤ë””ì˜¤ ë”ë¯¸ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.\n")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë””ì˜¤ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("âš ï¸  ì˜¤ë””ì˜¤ ë”ë¯¸ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.\n")
else:
    print("\nâš ï¸  TensorFlowê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì˜¤ë””ì˜¤ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜: pip install tensorflow\n")

# ==================== ì „ì²˜ë¦¬ ====================

# ë¹„ë””ì˜¤ í”„ë ˆì„ ì „ì²˜ë¦¬ (PyTorch)
image_transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

# ==================== í•¨ìˆ˜ ì •ì˜ ====================

def download_video_from_url(url):
    """yt-dlpë¥¼ ì‚¬ìš©í•˜ì—¬ URLì—ì„œ ì˜ìƒ ë‹¤ìš´ë¡œë“œ"""
    try:
        tmp_dir = tempfile.gettempdir()
        output_template = os.path.join(tmp_dir, 'downloaded_video.%(ext)s')
        
        command = [
            'yt-dlp',
            '-f', 'best[ext=mp4]/best',
            '-o', output_template,
            '--no-playlist',
            '--max-filesize', '100M',
            url
        ]
        
        print(f"  ğŸ”½ URLì—ì„œ ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì¤‘: {url}")
        
        result = subprocess.run(command, capture_output=True, text=True, timeout=300)
        
        if result.returncode != 0:
            raise Exception(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {result.stderr}")
        
        downloaded_files = [
            os.path.join(tmp_dir, f) 
            for f in os.listdir(tmp_dir) 
            if f.startswith('downloaded_video.')
        ]
        
        if downloaded_files:
            video_path = downloaded_files[0]
            print(f"  âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {video_path}")
            return video_path
        
        raise Exception("ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
    except subprocess.TimeoutExpired:
        raise Exception("ë‹¤ìš´ë¡œë“œ ì‹œê°„ ì´ˆê³¼ (5ë¶„)")
    except FileNotFoundError:
        raise Exception("yt-dlpê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install yt-dlp' ì‹¤í–‰ í•„ìš”")
    except Exception as e:
        raise Exception(f"ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {str(e)}")

def extract_frames(video_path, num_frames=30):
    """ë¹„ë””ì˜¤ì—ì„œ ê· ë“±í•˜ê²Œ í”„ë ˆì„ ì¶”ì¶œ"""
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    duration = total_frames / fps if fps > 0 else 0
    
    print(f"  ğŸ“Š ì˜ìƒ ì •ë³´: {total_frames}í”„ë ˆì„, {fps:.1f}fps, {duration:.1f}ì´ˆ")
    
    if total_frames == 0:
        raise ValueError("ë¹„ë””ì˜¤ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    frame_indices = np.linspace(0, total_frames - 1, min(num_frames, total_frames), dtype=int)
    frames = []
    
    for idx in frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if ret:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(frame)
    
    cap.release()
    print(f"  âœ… {len(frames)}ê°œ í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ")
    return frames

def extract_audio_to_spectrogram(video_path):
    """ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ í›„ Mel-spectrogram ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
    try:
        audio_path = video_path.replace(Path(video_path).suffix, '_audio.wav')
        
        # FFmpegë¡œ ì˜¤ë””ì˜¤ ì¶”ì¶œ
        command = [
            'ffmpeg',
            '-i', video_path,
            '-vn',
            '-acodec', 'pcm_s16le',
            '-ar', '22050',
            '-ac', '1',
            '-y',
            audio_path
        ]
        
        result = subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        
        if result.returncode != 0 or not os.path.exists(audio_path):
            print("  âš ï¸  ì˜¤ë””ì˜¤ ì¶”ì¶œ ì‹¤íŒ¨")
            return None
        
        # soundfile ì‚¬ìš© (aifc ëª¨ë“ˆ í•„ìš” ì—†ìŒ)
        try:
            import soundfile as sf
            y, sr = sf.read(audio_path)
            
            # ìŠ¤í…Œë ˆì˜¤ë©´ ëª¨ë…¸ë¡œ
            if len(y.shape) > 1:
                y = y.mean(axis=1)
            
            print(f"  âœ… ì˜¤ë””ì˜¤ ë¡œë“œ ì™„ë£Œ: {len(y)} samples, {sr}Hz")
            
        except ImportError as ie:
            print(f"  âš ï¸  soundfile ì—†ìŒ, ì„¤ì¹˜ í•„ìš”: pip install soundfile")
            if os.path.exists(audio_path):
                os.remove(audio_path)
            return None
        except Exception as load_error:
            print(f"  âš ï¸  ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨: {load_error}")
            if os.path.exists(audio_path):
                os.remove(audio_path)
            return None
        
        # Mel-spectrogram ìƒì„±
        mel_spec = librosa.feature.melspectrogram(
            y=y, 
            sr=sr, 
            n_mels=128,
            fmax=8000
        )
        
        # dB ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
        
        # ì •ê·œí™” (0~255 ë²”ìœ„)
        mel_spec_normalized = ((mel_spec_db - mel_spec_db.min()) / 
                               (mel_spec_db.max() - mel_spec_db.min()) * 255).astype(np.uint8)
        
        # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
        spec_image = Image.fromarray(mel_spec_normalized, mode='L')
        
        # ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ ì‚­ì œ
        if os.path.exists(audio_path):
            os.remove(audio_path)
        
        print(f"  âœ… Spectrogram ìƒì„± ì™„ë£Œ: {spec_image.size}")
        return spec_image
        
    except Exception as e:
        print(f"  âŒ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return None

def calculate_anomaly_score_pytorch(output, input_tensor):
    """PyTorch ëª¨ë¸ì˜ ì´ìƒ ìŠ¤ì½”ì–´ ê³„ì‚°"""
    reconstruction_error = torch.abs(output - input_tensor)
    anomaly_score = reconstruction_error.mean().item()
    fake_probability = min(anomaly_score * 100, 100)
    return fake_probability

def detect_deepfake_video(frames):
    """PyTorch GANomalyë¡œ ë¹„ë””ì˜¤ ë”¥í˜ì´í¬ íƒì§€"""
    
    if video_model is None:
        print("  âš ï¸  ë¹„ë””ì˜¤ ëª¨ë¸ ì—†ìŒ - ë”ë¯¸ ê°’ ì‚¬ìš©")
        return np.random.uniform(20, 80)
    
    try:
        predictions = []
        
        with torch.no_grad():
            for i, frame in enumerate(frames):
                frame_tensor = image_transform(frame).unsqueeze(0).to(device)
                
                try:
                    output = video_model(frame_tensor)
                    
                    if isinstance(output, tuple):
                        output = output[0]
                    
                    anomaly_score = calculate_anomaly_score_pytorch(output, frame_tensor)
                    predictions.append(anomaly_score)
                    
                except Exception as e:
                    print(f"    âš ï¸  í”„ë ˆì„ {i+1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    predictions.append(50.0)
        
        if not predictions:
            return np.random.uniform(20, 80)
        
        fake_probability = np.mean(predictions)
        print(f"  ğŸ“Š ë¹„ë””ì˜¤ ìŠ¤ì½”ì–´: min={min(predictions):.1f}%, max={max(predictions):.1f}%, avg={fake_probability:.1f}%")
        
        return float(fake_probability)
        
    except Exception as e:
        print(f"  âŒ ë¹„ë””ì˜¤ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return np.random.uniform(20, 80)

def detect_deepvoice_audio(spectrogram_image):
    """TensorFlow/Keras GANomalyë¡œ ì˜¤ë””ì˜¤ ë”¥ë³´ì´ìŠ¤ íƒì§€"""
    
    if audio_model is None:
        print("  âš ï¸  ì˜¤ë””ì˜¤ ëª¨ë¸ ì—†ìŒ - ë”ë¯¸ ê°’ ì‚¬ìš©")
        return np.random.uniform(20, 80)
    
    if spectrogram_image is None:
        print("  âš ï¸  Spectrogram ì—†ìŒ - ë¶„ì„ ìƒëµ")
        return 0.0
    
    try:
        # Spectrogramì„ ëª¨ë¸ ì…ë ¥ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (128x128, ì±„ë„ 1)
        spec_resized = spectrogram_image.resize((AUDIO_IMAGE_SIZE, AUDIO_IMAGE_SIZE))
        
        # numpy ë°°ì—´ë¡œ ë³€í™˜ (ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜)
        spec_array = np.array(spec_resized.convert('L'))  # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
        
        # ì •ê·œí™”: [0, 255] â†’ [-1, 1]
        spec_normalized = (spec_array.astype(np.float32) / 127.5) - 1.0
        
        # shape ì¡°ì •: (128, 128) â†’ (128, 128, 1) â†’ (1, 128, 128, 1)
        spec_normalized = np.expand_dims(spec_normalized, axis=-1)  # ì±„ë„ ì¶”ê°€
        spec_batch = np.expand_dims(spec_normalized, axis=0)  # ë°°ì¹˜ ì¶”ê°€
        
        # ëª¨ë¸ ì˜ˆì¸¡ - Generator ì§ì ‘ ì‚¬ìš©
        try:
            # ë‹¨ìˆœíˆ generatorë¡œ ì¬êµ¬ì„± ì´ë¯¸ì§€ ìƒì„±
            generated = audio_model.predict(spec_batch, verbose=0)
            
            # ì¬êµ¬ì„± ì˜¤ì°¨ ê³„ì‚° (L1 distance)
            reconstruction_error = np.abs(generated - spec_batch).mean()
            
            # anomaly score ê³„ì‚° (ë†’ì„ìˆ˜ë¡ ë”¥í˜ì´í¬ ê°€ëŠ¥ì„±)
            # í•™ìŠµ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™” (ì„ì˜ê°’, ì‹¤ì œë¡œëŠ” í•™ìŠµ ì‹œ ê³„ì‚°ëœ ê°’ ì‚¬ìš©)
            anomaly_score = np.clip(reconstruction_error * 1000, 0, 100)  # ìŠ¤ì¼€ì¼ ì¡°ì •
            
        except Exception as pred_error:
            print(f"    âš ï¸  ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜: {pred_error}")
            anomaly_score = 50.0
        
        print(f"  ğŸ“Š ì˜¤ë””ì˜¤ ìŠ¤ì½”ì–´: {anomaly_score:.1f}%")
        
        return float(anomaly_score)
        
    except Exception as e:
        print(f"  âŒ ì˜¤ë””ì˜¤ ë¶„ì„ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return np.random.uniform(20, 80)

# ==================== API ì—”ë“œí¬ì¸íŠ¸ ====================

@app.post("/api/analyze-url")
async def analyze_video_url(video_url: str = Form(...)):
    """URLë¡œ ì˜ìƒ ë¶„ì„"""
    
    print(f"\n{'='*60}")
    print(f"ğŸ”— URL ì˜ìƒ ë¶„ì„ ì‹œì‘")
    print(f"{'='*60}")
    print(f"  URL: {video_url}")
    
    video_path = None
    
    try:
        video_path = download_video_from_url(video_url)
        
        file_size_mb = os.path.getsize(video_path) / (1024 * 1024)
        print(f"  í¬ê¸°: {file_size_mb:.2f} MB")
        
        print("\nğŸ¬ [1/3] í”„ë ˆì„ ì¶”ì¶œ ì¤‘...")
        frames = extract_frames(video_path, num_frames=30)
        
        print("\nğŸ” [2/3] ë¹„ë””ì˜¤ ë”¥í˜ì´í¬ ë¶„ì„ ì¤‘...")
        video_deepfake_score = detect_deepfake_video(frames)
        
        print("\nğŸµ [3/3] ì˜¤ë””ì˜¤ ë”¥ë³´ì´ìŠ¤ ë¶„ì„ ì¤‘...")
        spectrogram = extract_audio_to_spectrogram(video_path)
        audio_deepfake_score = detect_deepvoice_audio(spectrogram)
        
        overall_score = (video_deepfake_score + audio_deepfake_score) / 2
        
        print(f"\n{'='*60}")
        print(f"âœ… ë¶„ì„ ì™„ë£Œ!")
        print(f"{'='*60}")
        print(f"  ğŸ¬ ë¹„ë””ì˜¤ ë”¥í˜ì´í¬: {video_deepfake_score:.2f}%")
        print(f"  ğŸµ ì˜¤ë””ì˜¤ ë”¥ë³´ì´ìŠ¤: {audio_deepfake_score:.2f}%")
        print(f"  ğŸ“Š ì¢…í•© ì ìˆ˜: {overall_score:.2f}%")
        print(f"{'='*60}\n")
        
        return JSONResponse(content={
            "success": True,
            "video_deepfake": float(video_deepfake_score),
            "audio_deepfake": float(audio_deepfake_score),
            "overall_score": float(overall_score),
            "frames_analyzed": len(frames),
            "audio_available": audio_model is not None and spectrogram is not None,
            "source": "url"
        })
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    finally:
        if video_path and os.path.exists(video_path):
            try:
                os.remove(video_path)
            except:
                pass

@app.post("/api/analyze")
async def analyze_video(video: UploadFile = File(...)):
    """íŒŒì¼ ì—…ë¡œë“œë¡œ ì˜ìƒ ë¶„ì„"""
    
    allowed_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv']
    file_ext = Path(video.filename).suffix.lower()
    
    if file_ext not in allowed_extensions:
        raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")
    
    with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp_video:
        content = await video.read()
        tmp_video.write(content)
        tmp_video_path = tmp_video.name
    
    try:
        print(f"\n{'='*60}")
        print(f"ğŸ“¹ íŒŒì¼ ë¶„ì„ ì‹œì‘: {video.filename}")
        print(f"{'='*60}")
        
        print("\nğŸ¬ [1/3] í”„ë ˆì„ ì¶”ì¶œ ì¤‘...")
        frames = extract_frames(tmp_video_path, num_frames=30)
        
        print("\nğŸ” [2/3] ë¹„ë””ì˜¤ ë”¥í˜ì´í¬ ë¶„ì„ ì¤‘...")
        video_deepfake_score = detect_deepfake_video(frames)
        
        print("\nğŸµ [3/3] ì˜¤ë””ì˜¤ ë”¥ë³´ì´ìŠ¤ ë¶„ì„ ì¤‘...")
        spectrogram = extract_audio_to_spectrogram(tmp_video_path)
        audio_deepfake_score = detect_deepvoice_audio(spectrogram)
        
        overall_score = (video_deepfake_score + audio_deepfake_score) / 2
        
        print(f"\n{'='*60}")
        print(f"âœ… ë¶„ì„ ì™„ë£Œ!")
        print(f"{'='*60}")
        print(f"  ğŸ¬ ë¹„ë””ì˜¤ ë”¥í˜ì´í¬: {video_deepfake_score:.2f}%")
        print(f"  ğŸµ ì˜¤ë””ì˜¤ ë”¥ë³´ì´ìŠ¤: {audio_deepfake_score:.2f}%")
        print(f"  ğŸ“Š ì¢…í•© ì ìˆ˜: {overall_score:.2f}%")
        print(f"{'='*60}\n")
        
        return JSONResponse(content={
            "success": True,
            "video_deepfake": float(video_deepfake_score),
            "audio_deepfake": float(audio_deepfake_score),
            "overall_score": float(overall_score),
            "frames_analyzed": len(frames),
            "audio_available": audio_model is not None and spectrogram is not None
        })
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    finally:
        if os.path.exists(tmp_video_path):
            os.remove(tmp_video_path)

@app.get("/")
async def root():
    return {
        "message": "ğŸ­ ë”¥í˜ì´í¬ íƒì§€ API",
        "status": "running",
        "models": {
            "video": {
                "loaded": video_model is not None,
                "framework": "PyTorch",
                "type": "GANomaly",
                "input_size": IMAGE_SIZE
            },
            "audio": {
                "loaded": audio_model is not None,
                "framework": "TensorFlow/Keras",
                "type": "GANomaly",
                "input_size": AUDIO_IMAGE_SIZE
            }
        },
        "device": str(device)
    }

@app.get("/health")
async def health_check():
    return {"status": "healthy"}

if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸš€ ë”¥í˜ì´í¬ íƒì§€ ì„œë²„ ì‹œì‘")
    print("="*60)
    print(f"ğŸ“Š ë¹„ë””ì˜¤ ëª¨ë¸: {'âœ… PyTorch GANomaly' if video_model else 'âŒ ì—†ìŒ'}")
    print(f"ğŸµ ì˜¤ë””ì˜¤ ëª¨ë¸: {'âœ… TensorFlow GANomaly' if audio_model else 'âŒ ì—†ìŒ'}")
    print(f"ğŸ’» ë””ë°”ì´ìŠ¤: {device}")
    print("="*60 + "\n")
    
    uvicorn.run(app, host="0.0.0.0", port=5000)